//- ============================================================================
//- Mathigon | Probability Content
//- (c) 2016 Mathigon
//- ============================================================================


// Sources
// * http://onlineroulette.org.uk/stories/karl-pearson/
// * http://www.eprisner.de/MAT109/FermatPascal.html
// * http://mathforum.org/isaac/problems/prob1.html
// * https://en.wikipedia.org/wiki/Penney%27s_game
// * https://en.wikipedia.org/wiki/Edward_O._Thorp
// * http://edwardothorp.com/id26.html


mixin frac(a,b,inline)
  if inline
    mfrac.inline
      mrow!= a
      mrow!= b
  else
    mfrac
      mrow!= a
      mrow!= b

mixin blank(choices)
  x-blank(choices=choices)

mixin input(answer)
  x-blank-input(solution=answer)

mixin var(bind)
  if bind
    x-var(bind=bind)
      | ${
      block
      | }
  else
    span.var
      | ${
      block
      | }

mixin barchart(data)
  - var max = 100/Math.max.apply(none, data.map(function(d) { return d[1] }))
  .barchart(style="width: " + data.length*50 + "px")
    for d in data
      .bar-wrap
        div(class="bar "+d[2], style="height: " + d[1]*max + "%")
    .axis
    for d in data
      .label= d[0]

- var reds = [32, 19, 21, 25, 34, 27, 36, 30, 23, 5, 16, 1, 14, 9, 18, 7, 12, 3]
- var colour = function(x) { return reds.indexOf(x) >= 0 ? 'r' : 'b'; }

mixin dice(n)
  svg(width=20, height=20)
    if n==1 || n==3 || n==5
      circle(r=2, cx=10, cy=10)
    if n==2 || n==3 || n==4 || n==5
      circle(r=2, cx=5, cy=5)
    if n==4 || n == 5
      circle(r=2, cx=5, cy=15)
    if n==4 || n == 5
      circle(r=2, cx=15, cy=5)
    if n==2 || n==3 || n==4 || n == 5
      circle(r=2, cx=15, cy=15)
    if n == 6
      circle(r=2, cx=5, cy=4)
      circle(r=2, cx=5, cy=10)
      circle(r=2, cx=5, cy=16)
      circle(r=2, cx=15, cy=4)
      circle(r=2, cx=15, cy=10)
      circle(r=2, cx=15, cy=16)

mixin emoji(n)
  img.emoji(width=20 height=20 src=`/images/emoji/${n}.png`)


//- ---------------------------------------------------------------------------
//- ---------------------------------------------------------------------------

x-step#intro

  h2 Introduction

  p Probabilities and likelihoods are everywhere around us, from weather forecasting to games, insurance or election polls. However, in the history of mathematics, probability it is actually a very recent idea. While geometry and algebra were studied by ancient Greek mathematicians more than 2500 years ago, the concepts of probability only emerged in the 17th and 18th century.

  .row
    .grow
      p According to legend, two of the greatest mathematicians, #[x-bio(xid='pascal') Blaise Pascal] and #[x-bio(xid='fermat') Pierre de Fermat], would regularly meet up in a small cafe in Paris.
      p To distract from the difficult mathematical theories they were discussing, they  often played a simple game: they repeatedly tossed a coin – every #[em heads] was a point for Pascal and every #[em tails] was a point for Fermat. Whoever had more points after three coin tosses had to pay the bill.
    div(style="width: 360px"): img(src="/resources/probability/images/paris.jpg", width="360", height="254")

  p One day, however, they get interrupted after the first coin toss and Fermat has to leave urgently. Later, they wonder who should pay the bill, or if there is a fair way to split it. The first coin landed #[em heads] (a point for Pascal), so maybe Fermat should pay everything. However, there is a small chance that Fermat could have still won if the #[+blank('two next tosses|next coin toss')] would have been #[em tails].

x-step#intro1
  p Pascal and Fermat decided to write down all possible ways the game could have continued:

  .row
    .grow.text-center(style="width: 120px")
      img(src="/resources/probability/images/coins-1.png" width=136 height=48 alt="HHH")
      p.caption Pascal wins
    .grow.text-center(style="width: 120px")
      img(src="/resources/probability/images/coins-2.png" width=136 height=48 alt="HHT")
      p.caption Pascal wins
    .grow.text-center(style="width: 120px")
      img(src="/resources/probability/images/coins-3.png" width=136 height=48 alt="HTH")
      p.caption Pascal wins
    .grow.text-center(style="width: 120px")
      img(src="/resources/probability/images/coins-4.png" width=136 height=48 alt="HTT")
      p.caption Fermat wins

  p All four possible outcomes are equally likely, and Pascal wins in #[+blank('three|four|two')] of them. #[span.reveal(data-when="blank-0") Thus they decided that Fermat should pay 3/4 of the bill and Pascal should pay 1/4.]


x-step#intro2
  p Pascal and Fermat had discovered the first important equation of probability: if an experiment has multiple possible outcomes which are all equally likely, then

  p.text-center Probability of an event = #[mfrac #[mrow Number of ways the event could happen]#[mrow Total number of possible outcomes]].

  p In our example, the probability of Pascal winning the game is #[+frac(3, 4, true)] = 0.75, and the probability of Fermat winning the game is #[+frac(1, 4, true)] = 0.25.

//- ---------------------------------------------------------------------------
//- ---------------------------------------------------------------------------

x-step#probLine
  h2 What are Probabilities

  p A #[strong probability] is a number between 0 and 1 which describes the likelihood of a certain #[strong event]. A probability of 0 means that something is #[em impossible]; a probability of 1 means that something is #[em certain].

  p For example, it is #[+blank('impossible|certain')] that you will meet a real life dragon, and it is #[+blank('certain|impossible')] that the sun will rise tomorrow. The probability of a coin landing #[em heads] is exactly #[+blank('in the middle|the same')].

  p.x-step-step(data-needs="blank-0 blank-1 blank-2") The probability of rolling a 6 on a die, or picking a particular suit from a deck of cards is #[+blank('less|more')] than 0.5 – which means unlikely. The probability of a good football team winning a match, or of a train arriving on time is #[+blank('more|less')] than 0.5 – which means likely.

  .p-line.clearfix
    img(src="/resources/probability/images/line-1.png", width=140, height=140, style="width: 18.42%; opacity: 0", alt="dragon")
    img(src="/resources/probability/images/line-2.png", width=140, height=140, style="width: 10.53%; opacity: 0", alt="dice")
    img(src="/resources/probability/images/line-3.png", width=140, height=140, style="width: 15.79%; opacity: 0", alt="cards")
    img(src="/resources/probability/images/line-4.png", width=140, height=140, style="width: 11.84%; opacity: 0", alt="coins")
    img(src="/resources/probability/images/line-5.png", width=140, height=140, style="width: 11.84%; opacity: 0", alt="football")
    img(src="/resources/probability/images/line-6.png", width=140, height=140, style="width: 17.11%; opacity: 0", alt="train")
    img(src="/resources/probability/images/line-7.png", width=140, height=140, style="width: 14.47%; opacity: 0", alt="sun")
    img(src="/resources/probability/images/line-8.png", width=760, height=40, style="width: 100%", alt="line")

x-step#probLine1
  p Here are some more events: drag them into the correct order, from likely to unlikely:

  x-sortable
    .item(data-index="3") You throw a die and it lands on 6.
    .item(data-index="5") Penguins #[+emoji('1f427')] live on the North Pole.
    .item(data-index="1") It's going to rain #[+emoji('26c8')] in November.
    .item(data-index="0") A baby will be born in China today.
    .item(data-index="4") You buy a lottery ticket and win the Jackpot #[+emoji('1f389')].
    .item(data-index="2") A newborn baby will be a girl #[+emoji('1f476')].

  p We often use probabilities and likelihoods in everyday life, usually without thinking about it. What is the chance of rain tomorrow? How likely is it that I will miss the bus? What is the probability I will win this game?


x-step#probLine2
  p Tossing a (fair) coin has two possible outcomes, #[em heads] and #[em tails], which are both equally likely. According to the equation above, the probability of a coin landing #[em heads] must be #[+frac(1, 2, true)] = 0.5, or 50%.
  //- TODO However, the equation is not very helpful if the different outcomes are not all equally likely, or if there are infinitely many possible outcomes.

  p Note that this probability is #[em in between] 0 and 1, even though only one of the outcomes can actually happen. But probabilities have very little to do with actual results: if we toss a coin many times we know that #[+blank('approximately half|exactly half|all|none')] of the results are heads – but we have no way of predicting #[em exactly which] tosses landed heads.

  p Even events with tiny probabilities (like winning the lottery #[+emoji('1f389')]) #[em can still happen] – and they #[em do happen] all the time (but to a very small proportion of the people who participate).

x-step#probLine3
  p Probabilities also depend on how much each of us knows about the event. For example, you might estimate that the chance of rain today is about 70%, while a meteorologist with detailed weather data might say the chance of rain is 64.2%.

  p Or suppose that I toss a coin and cover it up with my hands – the probability of tails is 50%. Now I peek at the result, but don't tell you. I know for certain what has happened, but for you the probability is #[+blank('still 50%|100%|not 50%')].

x-step#probLine4
  p There are many different ways to think about probabilities, but in practice they often give the same results:

  .row
    div(style="width: 240px")
      img(src="/resources/probability/images/classical.png" width=240 height=75 alt="classical probability")
      p.text-center The #[strong classical] probability of landing heads is the proportion of #[em possible outcomes] that are heads.
    div(style="width: 240px")
      img(src="/resources/probability/images/frequentist.png" width=240 height=75 alt="frequentist probability")
      p.text-center The #[strong frequentist] probability is the proportion of heads we get if we toss the coin #[em many times].
    div(style="width: 240px")
      img(src="/resources/probability/images/subjectivist.png" width=240 height=75 alt="subjectivist probability")
      p.text-center The #[strong subjectivist] probability tells us how strongly we #[em believe] that the coin will land heads.

  //- Notice that subjectivist probabilities may be different for different people – often depending on how much they know.

  p(style="margin-bottom: 2em") Remember that while probabilities are great for #[em estimating and forecasting], we can never tell what #[em actually] will happen.

  p Now let’s have a look at some fun applications of probability.


//- ---------------------------------------------------------------------------
//- ---------------------------------------------------------------------------

x-step#roulette.fill.dark.casino
  h2 Analysing Roulette

  .roulette-wheel
    .layer-2.wheel
    .layer-3
    .layer-4.wheel
    .layer-5
    .ball
    svg(width=380, height=380): circle(cx=190, cy=190, r=190)

  p Soon after their initial discovery, mathematicians started applying the laws of probability to many different parts of life – including casino games.

  p One of these mathematicians was #[x-bio(xid='pearson') Karl Pearson] who analysed the results of roulette games published in the French newspaper #[em Le Monaco].

  p Roulette consists of a wheel with the numbers from 1 to 36 coloured in #[strong.red red] and #[strong.black black], as well as a green 0. A ball rolls around the outside and randomly lands on one of the numbers. Gamblers can bet on a single number, a set of multiple numbers, or just a colour. Their potential winning depends on the likelihood of each of these outcomes.

x-step.fill.dark.casino#roulette1
  p Here is one of the many hundreds of newspaper extracts which Pearson collected and analysed. At first sight, it looks pretty random:

  .newspaper
    p Roulette results on 19 August 1823, Table 5:
    div
      for x in [13, 12, 30, 33, 3, 12, 29, 5, 8, 22, 23, 13, 5, 18, 14, 31, 36, 15, 18, 28, 32, 29, 11, 34, 23, 36, 8, 16, 2, 3, 9, 20, 16, 14, 15, 26, 31, 21, 15, 3, 33, 22, 12, 14, 9, 6, 30, 13, 33, 5, 28, 17, 27, 6, 5, 34, 11, 18, 32, 6, 9, 31, 29, 2, 18, 35, 6, 1, 34, 28, 1, 10]
        span(class=colour(x))= x

  p A roulette wheel has the same number of red and black numbers. If we ignore the green 0 (which means the casino wins) we would expect the number of red and black numbers to be #[+blank('approximately the same|exactly equal')]. Let’s check that this is indeed the case for the set of results above.

  +barchart([['Red', 37, 'r'], ['Black', 35, 'b']])

x-step.fill.dark.casino#roulette2
  p This looks pretty evenly distributed – there is a small difference between the number of red and black results, but that is always to be expected in probability.

  p However, Pearson didn’t stop here. He realised that if the results were completely random, then each of the four possible pairs of two consecutive colours should also be equally likely. Again we can count the number of occurrences in our example:

  +barchart([['RR', 14, 'r'], ['RB', 24, 'rb'], ['BR', 24, 'rb'], ['BB', 9, 'b']])

x-step.fill.dark.casino#roulette3
  p For some reasons, it seems that #[strong.red RR] and #[strong.black BB] happen much #[+blank('less frequently|more frequently')] than #[strong.red R]#[strong.black B] and #[strong.red B]#[strong.black R], even though they should all have the same probability. Of course, we might have just been #[em unlucky] in this particular sequence of results – but Pearson tested many thousands of results and always found the same.

x-step.fill.dark.casino#roulette4
  p It gets even worse if we look at triples of results. Each of the 8 possible triples of colours should be equally likely, but that is clearly not the case here:

  +barchart([['RRR', 3, 'r'], ['RRB', 10, 'rrb'], ['BRR', 10, 'rrb'], ['RBR', 15, 'rrb'], ['BRB', 14, 'bbr'], ['RBB', 8, 'bbr'], ['BBR', 8, 'bbr'], ['BBB', 2, 'b']])

  p It seems that in this particular casino, the colours alternate much more often than one would expect. There are hardly any long sequences of the same colour (#[strong.red RRR] or #[strong.black BBB]).

  p Pearson calculated that the probability of seeing results which were this skewed was less than 1 in 100,000,000! He assumed that the Roulette wheels were rigged to create higher profits for the Casino – and wrote many angry letters to expose this scam.

x-step.fill.dark.casino#roulette5
  .row
    div(style="width: 300px")
      x-media(src="/resources/probability/images/cocktails.jpg", credit="© Depositphotos / serrnovik", width=300, height=185)
    .grow
      p When he finally travelled to Monte Carlo, he discovered that the reason for the skewed results was of a very different nature: the journalists who were supposed to be recording the results were instead just sitting in the bar of the casino, drinking, and making up random colours…

x-step#randomSequence.fill.dark.casino(goals="random")
  p This story shows that we humans tend to be quite bad at coming up with random-looking data: we often underestimate unlikely events (long sequences of the same colour) and overestimate likely ones (alternating colours). This can be used effectively to detect fraud in banking and insurance.

  p Here you can try for yourself if you are better than the journalists: write down a sequence of Rs and Bs, and find out how random it really is:

  label.newspaper: input(type="text", placeholder="RBBRRBBBRRRBRBRRB")
  p.text-center(style="margin-top: -1em; font-family: monospace") Randomness Score: #[span.score 100]/100

x-step.fill.dark.casino#randomSequence1
  h2 Beat the Dealer

  .row
    .grow
      p While Pearson only analysed previous Roulette results, others tried to use mathematics to increase their chances of winning in casinos. One of these was #[x-bio(xid="thorp") Edward Thorp], who invented #[em card counting] – a technique that allowed him to beat casinos at #[x-gloss(xid="blackjack") Blackjack].

      p He later turned his focus to Roulette: believing that, if you knew the exact position and speed of the ball in a Roulette wheel, you should be able to use Physics to approximately predict the outcome. After the dealer sets the roulette wheel spinning, there are just a few seconds when you are still allowed to place new bets. Unfortunately this time is much too short for humans to calculate the outcome in their head.

    div(style="width: 150px")
      .book
        img(src="/resources/probability/images/beat-the-dealer.jpg", width=150, height=250)


x-step.fill.dark.casino#randomSequence2
  img.computer(src="/resources/probability/images/wearable-computer.png", width=275, height=364)

  p At the Massachusetts Institute of Technology, Thorp discussed his ideas with #[x-bio(xid="shannon") Claude Shannon], another mathematician and the father of #[x-gloss(xid="information") information theory]. Together they decided to build the first ever #[em wearable computer], decades before the likes of Google Glass or Apple Watch.

  p The computer was roughly the size of a pack of cigarettes and strapped around their waist. A set of wires ran down to their shoe, which they tapped whenever the ball crossed a certain marker on the roulette wheel. That allowed the computer to calculate its speed, and predict where it would end up. Another set of wires led from the computer to an earpiece, which produced different tones based on different outcomes.

x-step.fill.dark.casino#randomSequence3
  figure
    x-media(src="/resources/probability/images/las-vegas.jpg", credit="© Depositphotos / kobbydagan", width=760, height=345)

  p During the summer of 1961, Thorp and Shannon successfully tried their computer in Las Vegas. But while they made some money, the computer – which even contained parts of model airplanes – was not robust enough to be used at a larger scale.

  p Thorp wrote about their results in a scientific paper, and of course, computers were later forbidden in casinos. Thorp even got banned from all casinos in Las Vegas, but by then he had already moved on to yet more profitable ventures: using mathematics and computers on the stock market.

  //- p Shannon cryptography and code-breaking during World War II, and would go on to become known as the father of information theory -- and, indeed, the information age. Shannon taught him to juggle three balls, and that he rode a unicycle on a steel cable strung between two tree stumps. "He later reached his goal," he wrote, "which was to juggle the balls while riding the unicycle on the tightrope."

  p After this short trip through history, let’s get back to some actual mathematics…

//- ---------------------------------------------------------------------------
//- ---------------------------------------------------------------------------

x-step#trees
  h2 Probability Trees

  p In real life, coins never have exactly a probability of 0.5. It might be 0.4932 or 0.500012, depending on their exact shape or physical properties. In mathematics we don’t have to worry about these tiny inaccuracies: we can simply assume that our “mathematical model” of a coin has exactly a 0.5 probability of landing heads and is truly random. With this simplification, we can start answering much more interesting questions.

  p.todo More coming soon…

  //- div
    p.todo Now let’s try a more difficult game: there is a bag that contains five red and three blue marbles. When picking two marbles at random, what is the probability that both of them are red?

    p.todo We already know how to calculate the probability that the first marble is red: it is 5/8 = 0.625.

    p.todo However, once we have picked the first marble, the probabilities change: now there are only 7 marbles left, and only four of them are red. Therefore the probability that the second marble is red is 4/7 = 0.571.

    p.todo To calculate the probability that #[em both] marbles are red, we simply have to multiply these probabilities: the final answer is 0.625 × 0.571 = 0.357.

    p.todo There are four possible outcomes in total: red-red, red-blue, blue-red and blue-blue. We can represent all these possibilities in a single diagram:

    p.todo slideshow

    p.todo Now we can simply read off the probability of the different outcomes. The probability that both marbles are blue is xxx, and the probability that you get two marbles with different colours are xxx + xxx = xxx.

    p.todo Probability trees can be used to solve many problems that consist of multiple steps that happen one after the other. Here are some exercises for you to try:

  .box.problem-box
    .box-title: h3 Probability Trees
    .box-body: p.todo More coming soon…

//- ---------------------------------------------------------------------------
//- ---------------------------------------------------------------------------

x-step#venn
  h2 Venn Diagrams
  p.todo More coming soon…

  // p Opposite probabilities always add up to 1. This means that you can calculate the opposite of a probability by subtracting it from 1.

  // p Venn Diagrams and set operations

  // p Independent and Mutually Exclusive Events

  // p There are 200 kids in a school. 140 students are taking Mathematics, and 100 students are taking Biology. 80 students are studying both Maths and Biology.

  // p The corresponding Venn Diagram corresponds to two overlapping circles for Maths and Biology. We can write 80 in their interx-step.

  // p There are 140 - 80 = 60 students studying just Mathematics, so we write than in the remaining part of the Maths circle.  There are 100 - 80 = 20 students studying just Biology so we write that in the remaining part of the Biology circle.

  // p How many students at the school study neither Mathematics nor Biology?

//- ---------------------------------------------------------------------------
//- ---------------------------------------------------------------------------

x-step#future
  h2 Predicting the Future

  p If we roll two dice at once and add up their scores we could get results from #[+input(2)] up to #[+input(12)]. However, not all outcomes are equally likely: some results can only happen one way (for example, to get #[span.dice.outline 12] you have to roll #[span.dice #[+dice(6)]] + #[span.dice #[+dice(6)]]) while others can happen in multiple different ways (for example, to get #[span.dice.outline 5]) you could roll #[span.dice #[+dice(1)]] + #[span.dice #[+dice(4)]] or #[span.dice #[+dice(2)]] + #[span.dice #[+dice(3)]]).


x-step#futurea
  p This table shows all possible outcomes:

  table.dice-table
    tr
      td #[.dice.outline 2]
      td #[.dice.outline 3]
      td #[.dice.outline 4]
      td #[.dice.outline 5]
      td #[.dice.outline 6]
      td #[.dice.outline 7]
      td #[.dice.outline 8]
      td #[.dice.outline 9]
      td #[.dice.outline 10]
      td #[.dice.outline 11]
      td #[.dice.outline 12]
    tr
      td #[.dice #[+dice(1)]] #[.dice #[+dice(1)]]
      td #[.dice #[+dice(1)]] #[.dice #[+dice(2)]]
      td #[.dice #[+dice(1)]] #[.dice #[+dice(3)]]
      td #[.dice #[+dice(1)]] #[.dice #[+dice(4)]]
      td #[.dice #[+dice(1)]] #[.dice #[+dice(5)]]
      td #[.dice #[+dice(1)]] #[.dice #[+dice(6)]]
      td #[.dice #[+dice(2)]] #[.dice #[+dice(6)]]
      td #[.dice #[+dice(3)]] #[.dice #[+dice(6)]]
      td #[.dice #[+dice(4)]] #[.dice #[+dice(6)]]
      td #[.dice #[+dice(5)]] #[.dice #[+dice(6)]]
      td #[.dice #[+dice(6)]] #[.dice #[+dice(6)]]
    tr
      td
      td #[.dice #[+dice(2)]] #[.dice #[+dice(1)]]
      td #[.dice #[+dice(2)]] #[.dice #[+dice(2)]]
      td #[.dice #[+dice(2)]] #[.dice #[+dice(3)]]
      td #[.dice #[+dice(2)]] #[.dice #[+dice(4)]]
      td #[.dice #[+dice(2)]] #[.dice #[+dice(5)]]
      td #[.dice #[+dice(3)]] #[.dice #[+dice(5)]]
      td #[.dice #[+dice(4)]] #[.dice #[+dice(5)]]
      td #[.dice #[+dice(5)]] #[.dice #[+dice(5)]]
      td #[.dice #[+dice(6)]] #[.dice #[+dice(5)]]
      td
    tr
      td(colspan=2)
      td #[.dice #[+dice(3)]] #[.dice #[+dice(1)]]
      td #[.dice #[+dice(3)]] #[.dice #[+dice(2)]]
      td #[.dice #[+dice(3)]] #[.dice #[+dice(3)]]
      td #[.dice #[+dice(3)]] #[.dice #[+dice(4)]]
      td #[.dice #[+dice(4)]] #[.dice #[+dice(4)]]
      td #[.dice #[+dice(5)]] #[.dice #[+dice(4)]]
      td #[.dice #[+dice(6)]] #[.dice #[+dice(4)]]
      td(colspan=2)
    tr
      td(colspan=3)
      td #[.dice #[+dice(4)]] #[.dice #[+dice(1)]]
      td #[.dice #[+dice(4)]] #[.dice #[+dice(2)]]
      td #[.dice #[+dice(4)]] #[.dice #[+dice(3)]]
      td #[.dice #[+dice(5)]] #[.dice #[+dice(3)]]
      td #[.dice #[+dice(6)]] #[.dice #[+dice(3)]]
      td(colspan=3)
    tr
      td(colspan=4)
      td #[.dice #[+dice(5)]] #[.dice #[+dice(3)]]
      td #[.dice #[+dice(5)]] #[.dice #[+dice(2)]]
      td #[.dice #[+dice(6)]] #[.dice #[+dice(2)]]
      td(colspan=4)
    tr
      td(colspan=5)
      td #[.dice #[+dice(6)]] #[.dice #[+dice(1)]]
      td(colspan=5)


  p The most likely result when rolling two dice is #[span.dice.outline 7]. There are #[+input(6)] outcomes where the sum is 7, and #[+input(36)] outcomes in total, #[span.reveal(data-when="blank-0 blank-1") so the probability of getting a 7 is #[+frac(6, 36, true)] = 0.1666.]

  p The least likely outcomes are #[span.dice.outline 2] and #[span.dice.outline 12], each with a probability of #[+frac(1, 36, true)] = 0.0277.


x-step#diceSimulation(goals="roll")

  p It is impossible to forecast the outcome of a single coin toss or die roll. However, using probability we can very accurately predict the outcome of #[em many] dice.

  p If we throw a die 30 times, we know that we would get around #[+frac(1, 6, true)] × 30 = 5 sixes. If we roll it 300 times, there will be around #[+frac(1, 6, true)] × 300 = 50 sixes. These predictions get more and more accurate as we repeat the predictions more and more often.

  p In this animation you can roll many “virtual” dice at once and see how the results compare to the predicted probabilities:

  .box
    .box-title: h3 Rolling Dice
    .box-body
      .probTable.var ${ probTable(d) }

      p We roll #[+var('d|2|1,6,1') d] dice at once and record the #[span.dice(style="width: auto; padding: 0 4px;") SUM] of their scores. The #[strong.m-green green lines] represent the probabilities of every possible outcome predicted by probability theory and the #[strong.m-blue blue bars] show how often each outcome happened in this computer generated experiment.
      p.btn-row
        button.btn.btn-bounce Roll once
        button.btn.btn-bounce Roll 100 times
        button.btn.btn-bounce Roll 1000 times

  p Notice how, as we roll more and more dice, the observed frequencies become closer and closer to the frequencies we predicted using probability theory. This principle applies to all probability experiments and is called the #[strong Law of large numbers].

  //- TODO p Similarly, as we increase the number of dice rolled at once, you can also see that the probabilities change from a straight line (1 die) to a triangle (2 dice) and then to a bell-shaped curve. This is known as the #[strong Central Limit Theorem], and the bell-shaped curve is called the #[strong normal distribution].

//- ---------------------------------------------------------------------------
//- ---------------------------------------------------------------------------

x-step#montyhall.fill.dark.gameshow(goals="game")
  h2 Monty Hall

  p Welcome to the most spectacular game show on the planet! You now have a once-in-a-lifetime chance of winning a fantastic sports car which is hidden behind one of these three doors. Unfortunately, there are only goats behind the other two doors. Simply tap on one to make your choice!

  .monty-hall.selectable
    .door-box: .door
    .door-box: .door
    .door-box: .door
    .floor

  .reveal(data-when="door-select")
    p Are you sure about that? You can still change your mind by tapping a different door…
    p.text-center: button.btn.btn-bounce.sure I’m sure!

  .reveal(data-when="door-sure")
    p A great choice, but let me make life a little easier for you. I'll open one of the other doors with a goat, so that there are only two doors left for you to pick from. Do you want to stick with your choice, or do you want to swap?
    p.text-center
      button.btn.btn-bounce.swap I want to stay!
      button.btn.btn-bounce.swap I want to swap!

  .reveal(data-when="door-swap")
    p Ok – let's see how you did…
    p.text-center: button.btn.btn-bounce.show Open the doors…

  .reveal(data-when="door-revealed")
    p.monty-choice-right(style="display: none") Looks like swapping doors was a good choice. Congratulations, you just won a beautiful new sports car!
    p.monty-choice-wrong Sorry – it seems like time time you only won a goat. But don't worry, you can play again!
    p.text-center: button.btn.btn-bounce.reset Replay game

x-step.fill.dark.gameshow#montyhall1
  p If you play this game many times, you’ll notice that you’re more likely to win if you #[+blank('swap|don’t swap')] after the first door is opened#[span.required(data-needs="blank-0") , rather than sticking with your initial choice.]

  .reveal(data-when="blank-0")
    p But how can this be – surely the car is equally likely to be behind each of the two remaining doors?

x-step.fill.dark.gameshow#montyhall2
  p The explanation is very subtle. When you pick the initial door, the probability of being correct is #[+frac(1,3, true)] and the probability of being wrong is #[+frac(2,3, true)].
  p.text-center: include svg/monty-1.svg

x-step.fill.dark.gameshow#montyhall3
  p After the game master opens one of the other doors, the probability of being wrong is #[em still] #[+frac(2,3, true)], except now all this probability is on just one door. This means that swapping doors #[+blank('doubles|triples|halves')] your chance of winning.
  p.text-center: include svg/monty-2.svg

x-step.fill.dark.gameshow#montyhall4
  p Even if this doesn't seem very intuitive, we can prove that it is correct – simply by listing all different possibilities:
  figure: img(src="/resources/probability/images/monty.png", width="694", height="468")
  p Out of the 9 possibilities #[+input(6)] need you to switch doors, to win. This gives a chance of #[+frac(6,9, true)] = #[+frac(2,3, true)] like before.

//- ---------------------------------------------------------------------------
//- ---------------------------------------------------------------------------

x-step#quantum
  h2 True Randomness

  .row
    .grow
      p Most of this chapter relied on the fact that things like coins, or dice, or roulette wheels are completely random. However, that is not really true – we already learned that Edward Thorpe managed to predict the outcome of roulette.

      p Suppose we toss a coin: the chance of it landing heads is 0.5. If we knew which way the coin was facing just before it left the hand, we might be able to make a slightly better prediction, such as 0.58 or 0.41. If we also knew the weight and size of the coin, and the angle, position and speed as it left the hand, we could use the laws of physics – gravity, friction and air resistance – to model the motion of the coin and to predict the outcome. Finally, if we knew the exact position of every atom in the coin and of all the air molecules surrounding it, we could create a computer simulation to accurately predict what will happen.

    div(style="width: 240px")
      x-media(src="/resources/probability/images/coins.jpg", width=240, height=343, credit="© Depositphotos / Whiterabbit83")

x-step#quantum1
  p One could argue that tossing a coin really isn’t random at all – it is #[em chaotic]. That means that the underlying physical principles are so complex that even tiny changes to the starting conditions (speed, angle) can have a dramatic effect on the final outcome. We can use coins in games and gambling not because they are random, but because it is so incredibly difficult (and for practical purposes impossible) to predict the result.

  p The same principle applies to many other “random” events in life, including dice and roulette wheels. They are not really #[em random], we simply don’t have the tools to do the mathematical calculations accurately enough to predict the outcome.

x-step#radioactive(goals="decay")
  .row
    .grow
      p But #[em true randomness] does exists – at the very foundations of matter. A block of radioactive material consists of millions of atoms which decay over time: they fall apart into smaller atoms while emitting dangerous radiation.
      p Physicists know the probability that a particular atom will decay in a certain period of time. In fact, for a large block of radioactive material, the overall rate of decay is so steady that it is used in atomic clocks. But even knowing the exact properties of every atom, it is impossible to work out #[em which one] will decay next – this is completely random.

    div(style="width: 300px")
      p: svg.radioactive(width="300", height="200", viewBox="0 0 300 200")
      p.text-center.btn-row: button.btn.btn-bounce Start Decay

  //- TODO Possible probability distributions of the position of an electron in a hydrogen atom. Lighter areas represent more likely locations of the electron.

x-step#radioactive1
  p #[x-gloss(xid="radioactive") Radioactive decay] of atoms is caused by forces which act at much smaller scales within atoms, and which can be explained using #[x-gloss(xid="quantum") Quantum mechanics]. During the last century, physicists like #[x-bio(xid='planck') Max Planck] and #[x-bio(xid='dirac') Paul Dirac] discovered that fundamental particles have a mindblowing property: they can be in multiple different places #[em at the same time]. They don't have a fixed position, but instead a probability distribution (or wave function) which tells us how likely it is we are going to find them at a particular position.

  p This incredible property is used by Quantum computers. Conventional computers can only ever do one computation at a time. Quantum computers can use the properties of subatomic particles to do many calculations at the same time – and that makes them significantly faster.

  figure
    x-media(lightbox, src="/resources/probability/images/quantum.jpg", width="760", height="390", credit="© Depositphotos / agsandrew")
    p.caption The crazy world of subatomic particles

x-step#radioactive2
  p We can’t really #[em understand] or #[em explain] quantum mechanics – we just have to accept that it is what is predicted by mathematical theory and confirmed by physical observations. The curious quantum effects have only ever been observed on tiny scales of a few atoms, and it is not clear how they affect us in everyday life. But it is the only known effect in nature that produces #[em true randomness].

  //- TODO Lotto: Tickets needed to guarantee 2/3 matches
